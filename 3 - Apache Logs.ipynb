{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Server Log Analysis with Apache Spark\n",
    " \n",
    "This lab will demonstrate how easy it is to perform web server log analysis with Apache Spark.\n",
    "\n",
    "Server log analysis is an ideal use case for Spark.  It's a very large, common data source and contains a rich set of information. Spark allows you to store your logs in files on disk cheaply, while still providing a quick and simple way to perform data analysis on them. Log data comes from many sources, such as web, file, and compute servers, application logs, user-generated content, and can be used for monitoring servers, improving business and customer intelligence, building recommendation systems, fraud detection, and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is broken up into sections with bite-sized examples for demonstrating Spark functionality for log processing. For each problem, you should start by thinking about the algorithm that you will use to *efficiently* process the log in a parallel, distributed manner. This means using the various built in [pyspark functions](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) along with some [user defined functions](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf).\n",
    " \n",
    "This lab consists of 4 parts:\n",
    "\n",
    "1. Apache Web Server Log file format\n",
    "2. Sample Analyses on the Web Server Log File\n",
    "3. Analyzing Web Server Log File\n",
    "4. Exploring 404 Response Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Apache Web Server Log file format\n",
    "\n",
    "The log files that we use for this lab are in the [Apache Common Log Format (CLF)](http://httpd.apache.org/docs/1.3/logs.html#common). The log file entries produced in CLF will look something like this:\n",
    "`127.0.0.1 - - [01/Aug/1995:00:00:01 -0400] \"GET /images/launch-logo.gif HTTP/1.0\" 200 1839`\n",
    " \n",
    "Each part of this log entry is described below.\n",
    "\n",
    "* `127.0.0.1` - This is the IP address (or host name, if available) of the client (remote host) which made the request to the server.\n",
    " \n",
    "* `-` - The \"hyphen\" in the output indicates that the requested piece of information (user identity from remote machine) is not available.\n",
    " \n",
    "* `-` - The \"hyphen\" in the output indicates that the requested piece of information (user identity from local logon) is not available.\n",
    "\n",
    "* `[01/Aug/1995:00:00:01 -0400]` - The time that the server finished processing the request. The format is:\n",
    "`[day/month/year:hour:minute:second timezone]`\n",
    "\n",
    "  * day = 2 digits\n",
    "  * month = 3 letters\n",
    "  * year = 4 digits\n",
    "  * hour = 2 digits\n",
    "  * minute = 2 digits\n",
    "  * second = 2 digits\n",
    "  * zone = (+|-) 4 digits\n",
    " \n",
    "* `\"GET /images/launch-logo.gif HTTP/1.0\"` - This is the first line of the request string from the client. It consists of a three components:\n",
    "\n",
    "  * the request method (e.g., `GET`, `POST`, etc.)\n",
    "  * the endpoint (a [Uniform Resource Identifier](http://en.wikipedia.org/wiki/Uniform_resource_identifier))\n",
    "  * the client protocol and version\n",
    "\n",
    "* `200` - This is the status code that the server sends back to the client. This information is very valuable, because it reveals whether the request resulted in a successful response (codes beginning in 2), a redirection (codes beginning in 3), an error caused by the client (codes beginning in 4), or an error in the server (codes beginning in 5). The full list of possible status codes can be found in the HTTP specification ([RFC 2616](https://www.ietf.org/rfc/rfc2616.txt) section 10).\n",
    " \n",
    "* `1839` - The last entry indicates the size of the object returned to the client, not including the response headers. If no content was returned to the client, this value will be \"-\" (or sometimes 0).\n",
    " \n",
    "Note that log files contain information supplied directly by the client, without escaping. Therefore, it is possible for malicious clients to insert control-characters in the log files, *so care must be taken in dealing with raw logs.*\n",
    "\n",
    "For this lab, we will use a data set from NASA Kennedy Space Center WWW server in Florida. The full data set is freely available (http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html) and contains two months of HTTP requests. We are using a subset that only contains several days worth of requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1a) Parse a Log Timestamp\n",
    "\n",
    "We must first write a function that can parse an Apache Logs format timestamp. For this you should use `datetime.strptime()`. Passing a pattern string formatted according to [the documentation](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior), this can be used to build a `datetime` object from a string.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "pattern = '%d/%m/%y %H:%M'\n",
    "timestamp = '17/02/17 14:05'\n",
    "\n",
    "datetime.strptime(timestamp, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use this to define a function that parses an Apache log format timestamp to a `datetime` object (or `None` in the case of an invalid timestamp). Complete the time pattern to correctly parse Apache timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "APACHE_TIME_PATTERN = <FILL IN>\n",
    "\n",
    "def parse_apache_time(string):\n",
    "    \"\"\"Parse an Apache log formatted date string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    string : str\n",
    "        The string to parse\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    datetime or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(string, APACHE_TIME_PATTERN)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "parsed_time = parse_apache_time('01/Aug/1995:00:00:08 -0400')\n",
    "print(parsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from test_helper import Test\n",
    "from datetime import timedelta, timezone\n",
    "utc_minus_4 = timezone(timedelta(hours=-4))\n",
    "Test.assertEquals(parsed_time,\n",
    "                  datetime(1995, 8, 1, 0, 0, 8, tzinfo=utc_minus_4),\n",
    "                 'incorrect parsed_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1b) Extract components from log line\n",
    "\n",
    "Using the CLF as defined above, we write a regular expression pattern to extract the nine fields of the log line. This regular expression should extract 9 'groups' - one for each of:\n",
    "\n",
    "* Client host (hostname or IP address)\n",
    "* Remote user identity\n",
    "* Local user identity\n",
    "* Timestamp\n",
    "* Request method\n",
    "* Endpoint\n",
    "* Client protocol and version\n",
    "* Returned HTTP status code\n",
    "* Returned content size\n",
    "\n",
    "The function `parse_apache_log_line` applies the regular expression `APACHE_ACCESS_LOG_PATTERN` using the python [`re.search`](https://docs.python.org/3/library/re.html#re.search) method. Execute the cell to test the regular expression on a sample row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "APACHE_ACCESS_LOG_PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\" (\\d{3}) (\\S+)'\n",
    "\n",
    "APACHE_LOG_FIELD_NAMES = [\n",
    "    'host', 'client_identd', 'user_id', 'date_time', 'method', 'endpoint',\n",
    "    'protocol', 'response_code', 'content_size'\n",
    "]\n",
    "APACHE_ACCESS_LOG_SAMPLE_LINE = 'uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/ksclogo-medium.gif HTTP/1.0\" 304 0'\n",
    "\n",
    "\n",
    "def parse_apache_log_line(logline):\n",
    "    \"\"\"Parse a line in the Apache Common Log format.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logline : str\n",
    "        A line of text in the Apache Common Log format\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict or None\n",
    "        The parsed line\n",
    "    \"\"\"\n",
    "    \n",
    "    match = re.search(APACHE_ACCESS_LOG_PATTERN, logline)\n",
    "    \n",
    "    if match is None:\n",
    "        return\n",
    "    \n",
    "    fields = dict(zip(APACHE_LOG_FIELD_NAMES, match.groups()))\n",
    "\n",
    "    return fields\n",
    "\n",
    "\n",
    "pprint(parse_apache_log_line(APACHE_ACCESS_LOG_SAMPLE_LINE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(parse_apache_log_line(APACHE_ACCESS_LOG_SAMPLE_LINE),\n",
    "                  {'client_identd': '-', 'content_size': '0', 'date_time': '01/Aug/1995:00:00:08 -0400',\n",
    "                   'endpoint': '/images/ksclogo-medium.gif', 'host': 'uplherc.upl.com', 'method': 'GET',\n",
    "                   'protocol': 'HTTP/1.0', 'response_code': '304', 'user_id': '-'},\n",
    "                  'log line incorrectly parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1c) Configuration and Initial RDD Creation\n",
    "\n",
    "We are ready to specify the input log file and load it into a DataFrame.\n",
    "\n",
    "Let's start by fetching the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf apache.access.log.*\n",
    "!wget https://s3-eu-west-1.amazonaws.com/asi-training-data/spark/apache.access.log.PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the primary RDD that we'll use in the rest of this lab, we first load the text file using [`spark.read.text()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.text).\n",
    "\n",
    "Next, we use `.select()` on the DataFrame along with some [pyspark functions](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) and [user defined functions](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf) to generate a new DataFrame with the fields parsed into the desired formats.\n",
    "\n",
    "Finally, we cache the DataFrame in memory since we'll use it throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "APACHE_LOG_FILE = \"apache.access.log.PROJECT\"\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import types\n",
    "\n",
    "\n",
    "# A helper to generate a `.select()` entry that applies the access log\n",
    "# regular expression to the 'line' column and extracts a particular group as a new column\n",
    "extract_field = lambda group: func.regexp_extract('line', APACHE_ACCESS_LOG_PATTERN, group)\n",
    "\n",
    "# A helper function to apply parse_apache_time to a column\n",
    "parse_apache_time_udf = func.udf(parse_apache_time, types.TimestampType())\n",
    "\n",
    "# A helper function to cast a column to integer type\n",
    "cast_int = lambda col: col.cast('int')\n",
    "\n",
    "# A helper function to cast a column to integer type, interpreting '-' as 0\n",
    "safe_cast_int = lambda col: func.when(col == '-', 0).otherwise(col.cast('int'))\n",
    "    \n",
    "\n",
    "def parse_logs():\n",
    "    \"\"\"Load and parse the logs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.DataFrame\n",
    "        All the logs\n",
    "    pyspark.sql.DataFrame\n",
    "        The correctly parsed logs\n",
    "    pyspark.sql.DataFrame\n",
    "        The logs that could not be parsed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the dataset and rename the column to 'line'\n",
    "    logs = (spark.read.text(APACHE_LOG_FILE)\n",
    "            .select(func.col('value').alias('line')))\n",
    "    \n",
    "    # Parse the fields\n",
    "    parsed_logs = (logs\n",
    "                   .select(\n",
    "                       'line',  # Include the original line column\n",
    "                       extract_field(1).alias('host'),\n",
    "                       extract_field(2).alias('client_identd'),\n",
    "                       extract_field(3).alias('user_id'),\n",
    "                       parse_apache_time_udf(\n",
    "                           extract_field(4)\n",
    "                       ).alias('date_time'),\n",
    "                       extract_field(5).alias('method'),\n",
    "                       extract_field(6).alias('endpoint'),\n",
    "                       extract_field(7).alias('protocol'),\n",
    "                       cast_int(\n",
    "                           extract_field(8)\n",
    "                       ).alias('response_code'),\n",
    "                       safe_cast_int(\n",
    "                           extract_field(9)\n",
    "                       ).alias('content_size')\n",
    "                   )\n",
    "                   .cache())\n",
    "    \n",
    "    # Filter for correctly parsed logs\n",
    "    access_logs = parsed_logs.filter(func.length('host') > 0)\n",
    "    failed_logs = parsed_logs.filter(func.length('host') == 0)\n",
    "    \n",
    "    return parsed_logs, access_logs, failed_logs\n",
    "\n",
    "\n",
    "parsed_logs, access_logs, failed_logs = parse_logs()\n",
    "\n",
    "print('Total logs:          {}'.format(parsed_logs.count()))\n",
    "print('Successfully parsed: {}'.format(access_logs.count()))\n",
    "print('Failed to parse:     {}'.format(failed_logs.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1d) Data Cleaning\n",
    "\n",
    "Notice that there are a number of log lines that failed to parse. Since we included the original line column in the parsed DataFrames, we can inspect the lines we failed to pass with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('First 20 failed log lines:')\n",
    "for row in failed_logs.head(20):\n",
    "    print(repr(row['line']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the sample of invalid lines and compare them to the correctly parsed line. Based on your observations, alter the `APACHE_ACCESS_LOG_PATTERN` regular expression below so that the failed lines will correctly parse, and execute the cell below to rerun `parse_logs()`.\n",
    " \n",
    "If you not familar with Python regular expression [`search` function](https://docs.python.org/3/library/re.html#regular-expression-objects), now would be a good time to check up on the [documentation](https://developers.google.com/edu/python/regular-expressions). One tip that might be useful is to use an online tester like http://pythex.org or http://www.pythonregex.com. To use it, copy and paste the regular expression string below (located between the single quotes ') and test it against one of the 'Invalid logline' above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# This was originally '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\" (\\d{3}) (\\S+)'\n",
    "APACHE_ACCESS_LOG_PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\\s*\" (\\d{3}) (\\S+)'\n",
    "\n",
    "parsed_logs, access_logs, failed_logs = parse_logs()\n",
    "\n",
    "print('Total logs:          {}'.format(parsed_logs.count()))\n",
    "print('Successfully parsed: {}'.format(access_logs.count()))\n",
    "print('Failed to parse:     {}'.format(failed_logs.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(failed_logs.count(), 0, 'incorrect failed_logs.count()')\n",
    "Test.assertEquals(parsed_logs.count(), 1043177 , 'incorrect parsed_logs.count()')\n",
    "Test.assertEquals(access_logs.count(), parsed_logs.count(), 'incorrect access_logs.count()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Sample Analyses on the Web Server Log File\n",
    " \n",
    "Now that we have a DataFrame containing the components of the log file as columns, we can perform various analyses.\n",
    " \n",
    "### (2a) Example: Content Size Statistics\n",
    "\n",
    "Let's compute some statistics about the sizes of content being returned by the web server. In particular, we'd like to know what are the average, minimum, and maximum content sizes.\n",
    " \n",
    "We can compute the statistics by calling `.select()` with an aggregating [pyspark function](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) such as [`min()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.min), [`max()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.max) and [`avg()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.avg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_size = access_logs.select(func.min('content_size')).first()[0]\n",
    "max_size = access_logs.select(func.max('content_size')).first()[0]\n",
    "mean_size = access_logs.select(func.avg('content_size')).first()[0]\n",
    "\n",
    "print('Content size:')\n",
    "print('  min:  {}'.format(min_size))\n",
    "print('  max:  {}'.format(max_size))\n",
    "print('  mean: {}'.format(mean_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) Example: Response Code Analysis\n",
    "\n",
    "Next, lets look at the response codes that appear in the log. We'd like to count the number of times each response code occurs in the logs.\n",
    "\n",
    "To do this, first group the logs by response code, then count the size of each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_code_counts = (access_logs\n",
    "                        .groupBy('response_code')\n",
    "                        .count())\n",
    "response_code_counts.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Example: Response Code Graphing with `matplotlib`\n",
    "\n",
    "Now, lets visualize the results from the last example. We can visualize the results from the last example using [`matplotlib`](http://matplotlib.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandas_df = (response_code_counts\n",
    "             .orderBy('response_code')\n",
    "             .toPandas())\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.bar(np.arange(7), pandas_df['count'])\n",
    "\n",
    "ax.set_xticks(np.arange(7))\n",
    "ax.set_xticklabels(pandas_df['response_code'])\n",
    "\n",
    "ax.set_xlabel('Response code')\n",
    "ax.set_ylabel('Hits')\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2d) Example: Frequent Hosts\n",
    "\n",
    "Let's look at hosts that have accessed the server multiple times (e.g., more than ten times). As with the response code analysis in (2b), first we group by host, then we count the size of each group. Finally, we can apply a `.filter()` to return only hosts accessed the correct number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "host_more_than_10 = (access_logs\n",
    "                     .groupBy('host')\n",
    "                     .count()\n",
    "                     .filter(func.col('count') > 10))\n",
    "host_more_than_10.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(host_more_than_10.count(),\n",
    "                  23656,\n",
    "                  'incorrect size of host_more_than_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2e) Example: Visualizing Endpoints\n",
    "\n",
    "Now, lets visualize the number of hits to endpoints (URIs) in the log. To perform this task, we once again group and count to get the number of times each endpoint occurs in the logs, then order by descending number of hits. This data can then be plotted with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "endpoints = (access_logs\n",
    "             .groupBy('endpoint')\n",
    "             .count()\n",
    "             .orderBy('count', ascending=False))\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(12, 6))\n",
    "ax.plot(endpoints.toPandas()['count'])\n",
    "\n",
    "ax.set_xlabel('Endpoints')\n",
    "ax.set_ylabel('Number of Hits')\n",
    "ax.grid()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2f) Example: Top Endpoints\n",
    "\n",
    "For the final example, we'll look at the top endpoints (URIs) in the log. Since the DataFrame is already ordered by number of hits, we can just print the first ten lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Top ten endpoints:')\n",
    "for row in endpoints.head(10):\n",
    "    print('{} {}'.format(row['endpoint'], row['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEquals([row['endpoint'] for row in endpoints.head(10)],\n",
    "                  ['/images/NASA-logosmall.gif', \n",
    "                   '/images/KSC-logosmall.gif',\n",
    "                   '/images/MOSAIC-logosmall.gif',\n",
    "                   '/images/USA-logosmall.gif',\n",
    "                   '/images/WORLD-logosmall.gif',\n",
    "                   '/images/ksclogo-medium.gif',\n",
    "                   '/ksc.html',\n",
    "                   '/history/apollo/images/apollo-logo1.gif',\n",
    "                   '/images/launch-logo.gif',\n",
    "                   '/'],\n",
    "                  'incorrect top_ten_endpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Analyzing Web Server Log File\n",
    " \n",
    "Now it is your turn to perform analyses on web server log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Exercise: Top Ten Error Endpoints\n",
    "\n",
    "What are the top ten endpoints which did not have return code 200? Create a sorted list containing top ten endpoints and the number of times that they were accessed with non-200 return code.\n",
    " \n",
    "Think about the steps that you need to perform to determine which endpoints did not have a 200 return code, how you will uniquely count those endpoints, and sort the list.\n",
    " \n",
    "You might want to refer back to the previous Lab (Lab 1 Word Count) for insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# HINT: Each of these <FILL IN> below could be completed with a single transformation or action.\n",
    "# You are welcome to structure your solution in a different way, so long as\n",
    "# you ensure the variables used in the next Test section are defined (ie. not_200_counts, top_ten_endpoints_not_200).\n",
    "\n",
    "not_200 = access_logs.filter(func.col('response_code') != 200)\n",
    "\n",
    "not_200_counts = <FILL IN>\n",
    "\n",
    "sorted_not_200 = <FILL IN>\n",
    "\n",
    "top_ten_endpoints_not_200 = [row['endpoint'] for row in sorted_not_200.head(10)]\n",
    "\n",
    "print('Top ten non-200 status endpoints:')\n",
    "print('\\n'.join(top_ten_endpoints_not_200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Top ten error endpoints (3a)\n",
    "Test.assertEquals(not_200_counts.count(), 7689, 'incorrect count for not_200_counts')\n",
    "Test.assertEquals(top_ten_endpoints_not_200,\n",
    "                  ['/images/NASA-logosmall.gif',\n",
    "                   '/images/KSC-logosmall.gif',\n",
    "                   '/images/MOSAIC-logosmall.gif',\n",
    "                   '/images/USA-logosmall.gif',\n",
    "                   '/images/WORLD-logosmall.gif',\n",
    "                   '/images/ksclogo-medium.gif',\n",
    "                   '/history/apollo/images/apollo-logo1.gif',\n",
    "                   '/images/launch-logo.gif',\n",
    "                   '/',\n",
    "                   '/images/ksclogosmall.gif'],\n",
    "                  'incorrect top_ten_endpoints_not_200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Exercise: Number of Unique Hosts\n",
    "\n",
    "How many unique hosts are there in the entire log?\n",
    " \n",
    "Think about the steps that you need to perform to count the number of different hosts in the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# HINT: Do you recall the tips from (3a)? Each of these <FILL IN> could be an transformation or action.\n",
    "\n",
    "num_hosts = <FILL IN>\n",
    "\n",
    "print('Number of unique hosts: {}'.format(num_hosts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Number of unique hosts (3b)\n",
    "Test.assertEquals(num_hosts, 54507, 'incorrect num_hosts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Exercise: Number of Unique Daily Hosts\n",
    "\n",
    "For an advanced exercise, let's determine the number of unique hosts in the entire log on a day-by-day basis. This computation will give us counts of the number of unique daily hosts. We'd like a list sorted by increasing day of the month which includes the day of the month and the associated number of unique hosts for that day.\n",
    " \n",
    "Think about the steps that you need to perform to count the number of different hosts that make requests *each* day. You may find the [dayofmonth scala function](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofmonth) useful.\n",
    "\n",
    "*Since the log only covers a single month, you can ignore the month.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "grouped_by_day = <FILL IN>\n",
    "\n",
    "daily_hosts = <FILL IN>\n",
    "\n",
    "daily_hosts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Number of unique daily hosts (3c)\n",
    "Test.assertEquals(daily_hosts.count(), 22, 'incorrect unique_hosts_by_day.count()')\n",
    "Test.assertEquals(list(tuple(row) for _, row in daily_hosts.toPandas().iterrows()),\n",
    "                  [(1, 2582), (3, 2591), (4, 4262), (5, 2573), (6, 2469), (7, 4067),\n",
    "                   (8, 4259), (9, 4440), (10, 4432), (11, 4507), (12, 2865), (13, 2667),\n",
    "                   (14, 4363), (15, 4334), (16, 4253), (17, 4412), (18, 4231), (19, 2620),\n",
    "                   (20, 2482), (21, 4125), (22, 4416), (23, 696)],\n",
    "                  'incorrect daily_hosts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Exercise: Visualizing the Number of Unique Daily Hosts\n",
    "\n",
    "Using the results from the previous exercise, use matplotlib to plot a line graph of the unique hosts requests by day. `.toPandas()` is a useful method for converting a Spark DataFrame to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(figsize=(12, 6))\n",
    "\n",
    "<FILL IN>\n",
    "\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Hosts')\n",
    "ax.grid()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Exercise: Average Number of Daily Requests per Hosts\n",
    "\n",
    "Next, let's determine the average number of requests on a day-by-day basis. We'd like a list by increasing day of the month and the associated average number of requests per host for that day. Make sure you cache the resulting RDD `avgDailyReqPerHost` so that we can reuse it in the next exercise.\n",
    "\n",
    "To compute the average number of requests per host, get the total number of request across all hosts and divide that by the number of unique hosts.\n",
    "\n",
    "*Since the log only covers a single month, you can skip checking for the month. Also, to keep it simple, when calculating the approximate average use the integer value - you do not need to upcast to float*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "daily_host_requests = <FILL IN>\n",
    "\n",
    "average_daily_requests_per_host = <FILL IN>\n",
    "\n",
    "average_daily_requests_per_host.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Average number of daily requests per hosts (3e)\n",
    "Test.assertEquals(list((row[0], round(row[1], 2)) for _, row in average_daily_requests_per_host.toPandas().iterrows()),\n",
    "                  [(1.0, 13.17), (3.0, 12.74), (4.0, 14.30), (5.0, 12.59), (6.0, 13.08), (7.0, 13.87),\n",
    "                   (8.0, 13.58), (9.0, 13.91), (10.0, 13.82), (11.0, 13.84), (12.0, 12.88), (13.0, 13.9),\n",
    "                   (14.0, 13.59), (15.0, 13.89), (16.0, 13.09), (17.0, 13.42), (18.0, 13.56), (19.0, 12.5),\n",
    "                   (20.0, 13.03), (21.0, 13.4), (22.0, 12.97), (23.0, 10.89)],\n",
    "                  'incorrect average_daily_requests_per_host')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Exploring 404 Response Codes\n",
    " \n",
    "Let's drill down and explore the error 404 response code records. 404 errors are returned when an endpoint is not found by the server (i.e., a missing page or object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) Exercise: Counting 404 Response Codes\n",
    "\n",
    "Create a DataFrame containing only log records with a 404 response code. Make sure you `cache()` the DataFrame `bad_records` as we will use it in the rest of this exercise.\n",
    " \n",
    "How many 404 records are in the log?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "bad_records = <FILL IN>\n",
    "\n",
    "print('Found {} 404 URLs'.format(bad_records.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4b) Exercise: Listing 404 Response Code Records\n",
    "\n",
    "Using the DataFrame containing only log records with a 404 response code that you cached in part (4a), print out a list up to 40 **distinct** endpoints that generate 404 errors -  *no endpoint should appear more than once in your list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "bad_unique_endpoints = <FILL IN>\n",
    "\n",
    "bad_unique_endpoints.show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4c) Exercise: Listing the Top Twenty 404 Response Code Endpoints\n",
    "\n",
    "Using the DataFrame containing only log records with a 404 response code that you cached in part (4a), print out a list of the top twenty endpoints that generate the most 404 errors.\n",
    "\n",
    "*Remember, top endpoints should be in sorted order*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_endpoints_ranked = <FILL IN>\n",
    "\n",
    "bad_endpoints_ranked.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4d) Exercise: Listing the Top Twenty-five 404 Response Code Hosts\n",
    "\n",
    "Instead of looking at the endpoints that generated 404 errors, let's look at the hosts that encountered 404 errors. Using the DataFrame containing only log records with a 404 response code that you cached in part (4a), print out a list of the top twenty-five hosts that generate the most 404 errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_hosts_ranked = <FILL IN>\n",
    "\n",
    "bad_hosts_ranked.show(25, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4e) Exercise: Listing 404 Response Codes per Day\n",
    "\n",
    "Let's explore the 404 records temporally. Break down the 404 requests by day and get the daily counts sorted by day as a list.\n",
    "\n",
    "*Since the log only covers a single month, you can ignore the month in your checks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_records_per_day = <FILL IN>\n",
    "\n",
    "bad_records_per_day.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4f) Exercise: Visualizing the 404 Response Codes by Day\n",
    "\n",
    "Using the results from the previous exercise, use `matplotlib` to plot a \"Line\" or \"Bar\" graph of the 404 response codes by day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(figsize=(12, 8))\n",
    "\n",
    "<FILL IN>\n",
    "\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('404 Errors')\n",
    "ax.grid()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4g) Exercise: Hourly 404 Response Codes\n",
    "\n",
    "Using the DataFrame `bad_records` you cached in the part (4a) and by hour of the day and in increasing order, create a DataFrame containing how many requests had a 404 return code for each hour of the day (midnight starts at 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_records_by_hour = <FILL IN>\n",
    "\n",
    "bad_records_by_hour.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4h) Exercise: Visualizing the 404 Response Codes by Hour\n",
    "\n",
    "Using the results from the previous exercise, use `matplotlib` to plot a \"Line\" or \"Bar\" graph of the 404 response codes by hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(figsize=(12, 8))\n",
    "\n",
    "<FILL IN>\n",
    "\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('404 Errors')\n",
    "ax.grid()\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
